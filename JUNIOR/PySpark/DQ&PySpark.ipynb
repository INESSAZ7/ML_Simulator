{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a57d1c",
   "metadata": {},
   "source": [
    "# Data Quality               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa895d",
   "metadata": {},
   "source": [
    "## Step 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00751967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union, List\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metric:\n",
    "    \"\"\"Base class for Metric\"\"\"\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountTotal(Metric):\n",
    "    \"\"\"Total number of rows in DataFrame\"\"\"\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {\"total\": len(df)}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountZeros(Metric):\n",
    "    \"\"\"Number of zeros in choosen column\"\"\"\n",
    "\n",
    "    column: str\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column] == 0)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountNull(Metric):\n",
    "    \"\"\"Number of empty values in choosen columns\"\"\"\n",
    "\n",
    "    columns: List[str]\n",
    "    aggregation: str = \"any\"  # either \"all\", or \"any\"\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        \n",
    "        mask = df[self.columns[0]].isna()\n",
    "        if self.aggregation == \"any\":\n",
    "            for column in self.columns[1:]:\n",
    "                mask |= df[column].isna()\n",
    "        else:\n",
    "            for column in self.columns[1:]:\n",
    "                mask &= df[column].isna()\n",
    "        \n",
    "        k = sum(mask)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountDuplicates(Metric):\n",
    "    \"\"\"Number of duplicates in choosen columns\"\"\"\n",
    "\n",
    "    columns: List[str]\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df.duplicated(subset=self.columns))\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountValue(Metric):\n",
    "    \"\"\"Number of values in choosen column\"\"\"\n",
    "\n",
    "    column: str\n",
    "    value: Union[str, int, float]\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column] == self.value)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountBelowValue(Metric):\n",
    "    \"\"\"Number of values below threshold\"\"\"\n",
    "\n",
    "    column: str\n",
    "    value: float\n",
    "    strict: bool = False\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column] < self.value if self.strict else df[self.column] <= self.value)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountBelowColumn(Metric):\n",
    "    \"\"\"Count how often column X below Y\"\"\"\n",
    "\n",
    "    column_x: str\n",
    "    column_y: str\n",
    "    strict: bool = False\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column_x] < df[self.column_y] if self.strict else df[self.column_x] <= df[self.column_y])\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountRatioBelow(Metric):\n",
    "    \"\"\"Count how often X / Y below Z\"\"\"\n",
    "\n",
    "    column_x: str\n",
    "    column_y: str\n",
    "    column_z: str\n",
    "    strict: bool = False\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column_x]/df[self.column_y] < df[self.column_z] if self.strict \n",
    "                else df[self.column_x]/df[self.column_y] <= df[self.column_z])\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountCB(Metric):\n",
    "    \"\"\"Calculate lower/upper bounds for N%-confidence interval\"\"\"\n",
    "\n",
    "    column: str\n",
    "    conf: float = 0.95\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        alpha = 1 - self.conf\n",
    "        lcb, ucb = df[self.column].quantile([alpha/2, 1-alpha/2])\n",
    "        return {\"lcb\": lcb, \"ucb\": ucb}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountLag(Metric):\n",
    "    \"\"\"A lag between latest date and today\"\"\"\n",
    "\n",
    "    column: str\n",
    "    fmt: str = \"%Y-%m-%d\"\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        a = datetime.today ()\n",
    "        b = datetime.strptime(max(df[self.column]), self.fmt)\n",
    "        lag = a - b\n",
    "        return {\"today\": a.strftime(self.fmt), \"last_day\": b.strftime(self.fmt), \"lag\": lag.days}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c116167c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>item_id</th>\n",
       "      <th>qty</th>\n",
       "      <th>price</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-24</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>120.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-24</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>120.0</td>\n",
       "      <td>720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-24</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>200.0</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-24</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-23</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>110.0</td>\n",
       "      <td>330.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          day  item_id  qty  price  revenue\n",
       "0  2022-10-24      100    5  120.0    500.0\n",
       "1  2022-10-24      100    6  120.0    720.0\n",
       "2  2022-10-24      200    2  200.0    400.0\n",
       "3  2022-10-24      300   10   85.0    850.0\n",
       "4  2022-10-23      100    3  110.0    330.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales = pd.read_csv('ke_daily_sales.csv')\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab6210ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9 entries, 0 to 8\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   day       9 non-null      object\n",
      " 1   item_id   9 non-null      int64 \n",
      " 2   views     9 non-null      object\n",
      " 3   clicks    9 non-null      int64 \n",
      " 4   payments  9 non-null      int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 488.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_visits = pd.read_csv('ke_visits.csv')\n",
    "df_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a8f6b52",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f44af10e3a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcz_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountZeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'views'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mcz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_visits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcz_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'delta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ct_dict = {\"total\": 9}\n",
    "ct = CountTotal()\n",
    "assert ct(df_visits) == ct_dict\n",
    "\n",
    "cz_dict = {\"total\": 9, \"count\": 1, \"delta\": 1 / 9}\n",
    "cz = CountZeros(column='views')\n",
    "assert cz(df_visits) == cz_dict\n",
    "\n",
    "cn_dict = {'total': 9, 'count': 1, 'delta': 1/9}\n",
    "df_visits.loc[2,'views'] = None\n",
    "cn = CountNull(columns=['item_id', 'views'], aggregation=\"any\")\n",
    "assert cn(df_visits) == cn_dict\n",
    "\n",
    "#для проверки поменять 'today' на текущую дату\n",
    "cl_dict = {'today': '2023-10-09', 'last_day': '2022-10-24', 'lag': 350}\n",
    "cl = CountLag(column ='day')\n",
    "assert cl(df=df_sales) == cl_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808ae9e",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49824821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Report checklist.\"\"\"\n",
    "\n",
    "from metrics import (\n",
    "    CountTotal,\n",
    "    CountLag,\n",
    "    CountDuplicates,\n",
    "    CountNull,\n",
    "    CountRatioBelow,\n",
    "    CountCB,\n",
    "    CountZeros,\n",
    "    CountBelowValue,\n",
    "    CountBelowColumn,\n",
    ")\n",
    "\n",
    "# Checklist contains checks consist of:\n",
    "# - table_name\n",
    "# - metric\n",
    "# - limits\n",
    "\n",
    "CHECKLIST = [\n",
    "    # Table with sales [\"day\", \"item_id\", \"qty\", \"revenue\", \"price\"]\n",
    "    (\"sales\", CountTotal(), {\"total\": (1, 1e6)}),\n",
    "    (\"sales\", CountLag(\"day\"), {\"lag\": (0, 3)}),\n",
    "    (\"sales\", CountDuplicates([\"day\", \"item_id\"]), {\"total\": (0, 0)}),\n",
    "    (\"sales\", CountNull([\"qty\"]), {\"total\": (0, 0)}),\n",
    "    (\"sales\", CountRatioBelow(\"revenue\", \"price\", \"qty\", False), {\"delta\": (0, 0.05)}),\n",
    "    (\"sales\", CountCB(\"revenue\"), {}),\n",
    "    (\"sales\", CountZeros(\"qty\"), {\"delta\": (0, 0.3)}),\n",
    "    (\"sales\", CountBelowValue(\"price\", 100.0), {\"delta\": (0, 0.3)}),\n",
    "    # Table with clickstream [\"dt\", \"item_id\", \"views\", \"clicks\", \"payments\"]\n",
    "    (\"relevance\", CountTotal(), {\"total\": (1, 1e6)}),\n",
    "    (\"relevance\", CountLag(\"dt\"), {\"lag\": (0, 3)}),\n",
    "    (\"relevance\", CountZeros(\"views\"), {\"delta\": (0, 0.2)}),\n",
    "    (\"relevance\", CountZeros(\"clicks\"), {\"delta\": (0, 0.5)}),\n",
    "    (\"relevance\", CountNull([\"views\", \"clicks\", \"payments\"]), {\"delta\": (0, 0.1)}),\n",
    "    (\"relevance\", CountBelowValue(\"views\", 10), {\"delta\": (0, 0.5)}),\n",
    "    (\"relevance\", CountBelowColumn(\"clicks\", \"views\"), {\"total\": (0, 0)}),\n",
    "    (\"relevance\", CountBelowColumn(\"payments\", \"clicks\"), {\"total\": (0, 0)}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46ff0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DQ Report.\"\"\"\n",
    "\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "from dataclasses import dataclass\n",
    "#from user_input.metrics import Metric\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "LimitType = Dict[str, Tuple[float, float]]\n",
    "CheckType = Tuple[str, Metric, LimitType]\n",
    "\n",
    "\n",
    "\n",
    "def memoize(func: Callable) -> Callable:\n",
    "    \"\"\"Memoize function\"\"\"\n",
    "    cache = {}\n",
    "    def wrapped(*argv, **kwargs):\n",
    "        #print(cache)\n",
    "        key = str(argv) + str(kwargs)\n",
    "        if key not in cache:\n",
    "            cache[key] = func(*argv, **kwargs)\n",
    "        \n",
    "        return cache[key]\n",
    "    return wrapped\n",
    "\n",
    "@dataclass\n",
    "class Report:\n",
    "    \"\"\"DQ report class.\"\"\"\n",
    "\n",
    "    checklist: List[CheckType]\n",
    "    engine: str = \"pandas\"\n",
    "    \n",
    "    @memoize\n",
    "    def fit(self, tables: Dict[str, pd.DataFrame]) -> Dict:\n",
    "        \"\"\"Calculate DQ metrics and build report.\"\"\"\n",
    "        self.report_ = {}\n",
    "        report = self.report_\n",
    "\n",
    "        # Check if engine supported\n",
    "        if self.engine != \"pandas\":\n",
    "            raise NotImplementedError(\"Only pandas API currently supported!\")\n",
    "        \n",
    "        result = []\n",
    "        passed = 0\n",
    "        failed = 0\n",
    "        errors = 0\n",
    "        for table_name, metric, limits in self.checklist:\n",
    "            df = tables[table_name]\n",
    "            error = ''\n",
    "            try:\n",
    "                values = metric(df)\n",
    "                if len(limits) == 0:\n",
    "                    status = '.'\n",
    "                    passed+=1\n",
    "                else:\n",
    "                    # limits: {\"total\": (1, 1e6), \"\": () ... }\n",
    "                    for column_limit, limit in limits.items():\n",
    "                        value = values[column_limit]\n",
    "                        if value >= limit[0] and value <= limit[1]:\n",
    "                            status = '.'\n",
    "                            passed+=1\n",
    "                        else:\n",
    "                            status = 'F'\n",
    "                            failed+=1\n",
    "                  \n",
    "            except Exception as e:\n",
    "                status = 'E'\n",
    "                error = type(e).__name__ \n",
    "                errors+=1\n",
    "        \n",
    "            result.append({'table_name': table_name,\n",
    "                           'metric': str(metric),\n",
    "                           'limits': str(limits),\n",
    "                           'values': values,\n",
    "                           'status': status,\n",
    "                           'error': error})\n",
    "        \n",
    "        \n",
    "        report['title'] = 'DQ Report for tables ' + str(sorted(list(tables.keys())))\n",
    "        report['result'] = pd.DataFrame(result)\n",
    "        report['total'] = len(result)\n",
    "        report['passed'] = passed\n",
    "        report['passed_pct'] = round(passed / report['total'] * 100, 2)\n",
    "        report['failed'] = failed\n",
    "        report['failed_pct'] = round(failed / report['total'] * 100, 2)\n",
    "        report['errors'] = errors\n",
    "        report['errors_pct'] = round(errors / report['total'] * 100, 2)\n",
    "        \n",
    "\n",
    "        return report\n",
    "\n",
    "    def to_str(self) -> None:\n",
    "        \"\"\"Convert report to string format.\"\"\"\n",
    "        report = self.report_\n",
    "\n",
    "        msg = (\n",
    "            \"This Report instance is not fitted yet. \"\n",
    "            \"Call 'fit' before using this method.\"\n",
    "        )\n",
    "\n",
    "        assert isinstance(report, dict), msg\n",
    "\n",
    "        pd.set_option(\"display.max_rows\", 500)\n",
    "        pd.set_option(\"display.max_columns\", 500)\n",
    "        pd.set_option(\"display.max_colwidth\", 20)\n",
    "        pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "        return (\n",
    "            f\"{report['title']}\\n\\n\"\n",
    "            f\"{report['result']}\\n\\n\"\n",
    "            f\"Passed: {report['passed']} ({report['passed_pct']}%)\\n\"\n",
    "            f\"Failed: {report['failed']} ({report['failed_pct']}%)\\n\"\n",
    "            f\"Errors: {report['errors']} ({report['errors_pct']}%)\\n\"\n",
    "            \"\\n\"\n",
    "            f\"Total: {report['total']}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c56ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = Report(CHECKLIST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39828952",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b88ff0d55a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sales'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf_sales\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relevance'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf_visits\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-838aee34af4e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, tables)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pandas\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pyspark\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-838aee34af4e>\u001b[0m in \u001b[0;36m_fit_pandas\u001b[0;34m(self, tables)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_pyspark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-838aee34af4e>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, tables)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchecker_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimits\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecklist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "cl.fit({'sales': df_sales, 'relevance': df_visits})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd46043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DQ Report for tables ['relevance', 'sales']\\n\\n   table_name               metric               limits               values status     error\\n0       sales         CountTotal()  {'total': (1, 10...         {'total': 7}      .          \\n1       sales  CountLag(column=...      {'lag': (0, 3)}  {'today': '2023-...      F          \\n2       sales  CountDuplicates(...    {'total': (0, 0)}  {'total': 7, 'co...      F          \\n3       sales  CountNull(column...    {'total': (0, 0)}  {'total': 7, 'co...      F          \\n4       sales  CountRatioBelow(...  {'delta': (0, 0....  {'total': 7, 'co...      F          \\n5       sales  CountCB(column='...                   {}  {'lcb': 49.50000...      .          \\n6       sales  CountZeros(colum...  {'delta': (0, 0.3)}  {'total': 7, 'co...      .          \\n7       sales  CountBelowValue(...  {'delta': (0, 0.3)}  {'total': 7, 'co...      .          \\n8   relevance         CountTotal()  {'total': (1, 10...         {'total': 9}      .          \\n9   relevance  CountLag(column=...      {'lag': (0, 3)}         {'total': 9}      E  KeyError\\n10  relevance  CountZeros(colum...  {'delta': (0, 0.2)}  {'total': 9, 'co...      .          \\n11  relevance  CountZeros(colum...  {'delta': (0, 0.5)}  {'total': 9, 'co...      .          \\n12  relevance  CountNull(column...  {'delta': (0, 0.1)}  {'total': 9, 'co...      F          \\n13  relevance  CountBelowValue(...  {'delta': (0, 0.5)}  {'total': 9, 'co...      .          \\n14  relevance  CountBelowColumn...    {'total': (0, 0)}  {'total': 9, 'co...      F          \\n15  relevance  CountBelowColumn...    {'total': (0, 0)}  {'total': 9, 'co...      F          \\n\\nPassed: 8 (50.0%)\\nFailed: 7 (43.75%)\\nErrors: 1 (6.25%)\\n\\nTotal: 16\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.to_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a949a",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef6754",
   "metadata": {},
   "source": [
    "Документация с примерами\n",
    "- https://sparkbyexamples.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "48847ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 9\n",
      "root\n",
      " |-- day: string (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- clicks: integer (nullable = true)\n",
      " |-- payments: integer (nullable = true)\n",
      "\n",
      "+----------+-------+-----+------+--------+\n",
      "|       day|item_id|views|clicks|payments|\n",
      "+----------+-------+-----+------+--------+\n",
      "|2022-09-24|    100| 1000|   219|      56|\n",
      "|2022-09-24|    200| 1248|   343|       1|\n",
      "|2022-09-24|    300|  993|   102|      71|\n",
      "|2022-09-23|    100| 3244|   730|      18|\n",
      "|2022-09-23|    200|  830|   203|       9|\n",
      "|2022-09-23|    300|    0|     0|       2|\n",
      "|2022-09-22|    100| NULL|   123|      20|\n",
      "|2022-09-22|    200| 5320|   500|      13|\n",
      "|2022-09-22|    300|  777|    68|       2|\n",
      "+----------+-------+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the PySpark libraries\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# Create a SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# Load a CSV file\n",
    "df = sqlContext.read.csv(\"ke_visits.csv\", header=True)\n",
    "df_relevance = sqlContext.read.csv(\"ke_daily_sales.csv\", header=True)\n",
    "\n",
    "# Count the number of rows in the DataFrame\n",
    "rowCount = df.count()\n",
    "\n",
    "# Print the row count\n",
    "print(\"Number of rows:\", rowCount)\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "df = df.withColumn(\"views\", df[\"views\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"clicks\", df[\"clicks\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"item_id\", df[\"item_id\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"payments\", df[\"payments\"].cast(IntegerType()))\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Show the first 10 rows of the DataFrame\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "691fc1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union, List\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql as ps\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metric:\n",
    "    \"\"\"Base class for Metric\"\"\"\n",
    "\n",
    "    def __call__(self, df: Union[pd.DataFrame, ps.DataFrame]) -> Dict[str, Any]:\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            return self._call_pandas(df)\n",
    "\n",
    "        if isinstance(df, ps.DataFrame):\n",
    "            return self._call_pyspark(df)\n",
    "\n",
    "        msg = (\n",
    "            f\"Not supported type of arg 'df': {type(df)}. \"\n",
    "            \"Supported types: pandas.DataFrame, \"\n",
    "            \"pyspark.sql.dataframe.DataFrame\"\n",
    "        )\n",
    "        raise NotImplementedError(msg)\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountTotal(Metric):\n",
    "    \"\"\"Total number of rows in DataFrame\"\"\"\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {\"total\": len(df)}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        return {\"total\": df.count()}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountZeros(Metric):\n",
    "    \"\"\"Number of zeros in choosen column\"\"\"\n",
    "\n",
    "    column: str\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column] == 0)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col, count\n",
    "\n",
    "        n = df.count()\n",
    "        k = df.filter(col(self.column) == 0).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountNull(Metric):\n",
    "    \"\"\"Number of empty values in choosen columns\"\"\"\n",
    "\n",
    "    columns: List[str]\n",
    "    aggregation: str = \"any\"  # either \"all\", or \"any\"\n",
    "    \n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        mask = df[self.columns[0]].isna()\n",
    "        if self.aggregation == \"any\":\n",
    "            for column in self.columns[1:]:\n",
    "                mask |= df[column].isna()\n",
    "        else:\n",
    "            for column in self.columns[1:]:\n",
    "                mask &= df[column].isna()\n",
    "        \n",
    "        k = sum(mask)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col, count, isnan\n",
    "\n",
    "        n = df.count()\n",
    "        mask = col(self.columns[0]).isNull() |  isnan(col(self.columns[0]))\n",
    "        if self.aggregation == \"any\":\n",
    "            for column in self.columns[1:]:\n",
    "                c = col(column)\n",
    "                mask = mask | (c.isNull() |  isnan(c))\n",
    "        else:\n",
    "            for column in self.columns[1:]:\n",
    "                c = col(column)\n",
    "                mask = mask & (c.isNull() |  isnan(c))\n",
    "        \n",
    "        k = df.filter(mask).count()        \n",
    "                \n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class CountDuplicates(Metric):\n",
    "    \"\"\"Number of duplicates in choosen columns\"\"\"\n",
    "\n",
    "    columns: List[str]\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df.duplicated(subset=self.columns))\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "    \n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col, count\n",
    "        n = df.count()\n",
    "        k = df.groupby(self.columns).count().where(col('count') > '1').count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountValue(Metric):\n",
    "    \"\"\"Number of values in choosen column\"\"\"\n",
    "\n",
    "    column: str\n",
    "    value: Union[str, int, float]\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column] == self.value)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "    \n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "        n = df.count()\n",
    "        k = df.filter(col(self.column) == self.value).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountBelowValue(Metric):\n",
    "    \"\"\"Number of values below threshold\"\"\"\n",
    "\n",
    "    column: str\n",
    "    value: float\n",
    "    strict: bool = False\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column] < self.value if self.strict else df[self.column] <= self.value)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "    \n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "        n = df.count()\n",
    "        if self.strict:   \n",
    "            k = df.filter(col(self.column) < self.value).count()\n",
    "        else:\n",
    "            k = df.filter(col(self.column) <= self.value).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountBelowColumn(Metric):\n",
    "    \"\"\"Count how often column X below Y\"\"\"\n",
    "\n",
    "    column_x: str\n",
    "    column_y: str\n",
    "    strict: bool = False\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column_x] < df[self.column_y] if self.strict else df[self.column_x] <= df[self.column_y])\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "    \n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col, isnan\n",
    "        n = df.count()\n",
    "        mask_x = ~isnan(col(self.column_x)) & col(self.column_x).isNotNull()\n",
    "        mask_y = ~isnan(col(self.column_y)) & col(self.column_y).isNotNull()\n",
    "        if self.strict:   \n",
    "            k = df.filter(mask_x & mask_y & (col(self.column_x) < col(self.column_y))).count()\n",
    "        else:\n",
    "            k = df.filter(mask_x & mask_y & (col(self.column_x) <= col(self.column_y))).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountRatioBelow(Metric):\n",
    "    \"\"\"Count how often X / Y below Z\"\"\"\n",
    "\n",
    "    column_x: str\n",
    "    column_y: str\n",
    "    column_z: str\n",
    "    strict: bool = False\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column_x]/df[self.column_y] < df[self.column_z] if self.strict \n",
    "                else df[self.column_x]/df[self.column_y] <= df[self.column_z])\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "    \n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col, isnan\n",
    "        n = df.count()\n",
    "        ration = col(self.column_x)/col(self.column_y)\n",
    "        mask_x = ~isnan(col(self.column_x)) & col(self.column_x).isNotNull()\n",
    "        mask_y = ~isnan(col(self.column_y)) & col(self.column_y).isNotNull()\n",
    "        mask_z = ~isnan(col(self.column_z)) & col(self.column_z).isNotNull()\n",
    "        mask = mask_x & mask_y & mask_z\n",
    "        if self.strict:   \n",
    "            k = df.filter(mask & (ration < col(self.column_z))).count()\n",
    "        else:\n",
    "            k = df.filter(mask & (ration <= col(self.column_z))).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountCB(Metric):\n",
    "    \"\"\"Calculate lower/upper bounds for N%-confidence interval\"\"\"\n",
    "\n",
    "    column: str\n",
    "    conf: float = 0.95\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        alpha = 1 - self.conf\n",
    "        lcb, ucb = df[self.column].quantile([alpha/2, 1-alpha/2])\n",
    "        return {\"lcb\": lcb, \"ucb\": ucb}\n",
    "    \n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import percentile_approx\n",
    "        alpha = 1 - self.conf\n",
    "        q = df.select(percentile_approx(self.column, [alpha/2, 1-alpha/2])).collect()[0][0]\n",
    "        return {\"lcb\": q[0], \"ucb\": q[1]}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountLag(Metric):\n",
    "    \"\"\"A lag between latest date and today\"\"\"\n",
    "    \n",
    "    column: str\n",
    "    fmt: str = \"%Y-%m-%d\"\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        a = datetime.today ()\n",
    "        b = datetime.strptime(max(df[self.column]), self.fmt)\n",
    "        lag = a - b\n",
    "        return {\"today\": a.strftime(self.fmt), \"last_day\": b.strftime(self.fmt), \"lag\": lag.days}\n",
    "\n",
    "    def _call_pyspark(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import max\n",
    "        a = datetime.today ()\n",
    "        max_time = df.select(max(self.column)).collect()[0][0]\n",
    "        b = datetime.strptime(max_time, self.fmt)\n",
    "        lag = a - b\n",
    "        return {\"today\": a.strftime(self.fmt), \"last_day\": b.strftime(self.fmt), \"lag\": lag.days}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30558310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверки\n",
    "\n",
    "df = df.replace([123, 1000], [None, None])\n",
    "cn_dict = {'total': 9, 'count': 2, 'delta': 2/9}\n",
    "cn = CountNull(columns=['clicks', 'views'], aggregation=\"any\")\n",
    "assert cn(df) == cn_dict\n",
    "\n",
    "new_df = spark.createDataFrame([('2022-09-24', '200', '1248', '343', '1')], df.columns)\n",
    "df = df.union(new_df)\n",
    "df = df.withColumn(\"views\", df[\"views\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"clicks\", df[\"clicks\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"item_id\", df[\"item_id\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"payments\", df[\"payments\"].cast(IntegerType()))\n",
    "cd_dict = {'total': 10, 'count': 1, 'delta': 1/10}\n",
    "cd = CountDuplicates(['clicks', 'views'])\n",
    "assert cd(df) == cd_dict\n",
    "\n",
    "\n",
    "cv_dict = {'total': 10, 'count': 2, 'delta': 2/10}\n",
    "cv = CountValue('clicks', 343)\n",
    "assert cv(df) == cv_dict\n",
    "\n",
    "cbv_dict = {'total': 10, 'count': 7, 'delta': 7/10}\n",
    "cbv = CountBelowValue('clicks', 343)\n",
    "assert cbv(df) == cbv_dict\n",
    "\n",
    "cbc_dict = {'total': 10, 'count': 1, 'delta': 1/10}\n",
    "cbc = CountBelowColumn('views', 'clicks')\n",
    "assert cbc(df) == cbc_dict\n",
    "\n",
    "ccb_dict = {'lcb': 0, 'ucb': 730}\n",
    "ccb = CountCB('clicks')\n",
    "assert ccb(df) == ccb_dict\n",
    "\n",
    "cl_dict = {'today': '2023-10-20', 'last_day': '2022-09-24', 'lag': 391}\n",
    "cl = CountLag(column ='day')\n",
    "assert cl(df=df) == cl_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ce701e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "from dataclasses import dataclass\n",
    "#from user_input.metrics import Metric\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql as ps\n",
    "\n",
    "LimitType = Dict[str, Tuple[float, float]]\n",
    "CheckType = Tuple[str, Metric, LimitType]\n",
    "\n",
    "\n",
    "def memoize(func: Callable) -> Callable:\n",
    "    \"\"\"Memoize function\"\"\"\n",
    "    cache = {}\n",
    "    def wrapped(*argv, **kwargs):\n",
    "        key = str(argv) + str(kwargs)\n",
    "        if key not in cache:\n",
    "            cache[key] = func(*argv, **kwargs)\n",
    "        \n",
    "        return cache[key]\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Report:\n",
    "    \"\"\"DQ report class.\"\"\"\n",
    "\n",
    "    checklist: List[CheckType]\n",
    "    engine: str = \"pandas\"\n",
    "    \n",
    "    \n",
    "    def fit(self, tables: Dict[str, Union[pd.DataFrame, ps.DataFrame]]) -> Dict:\n",
    "        \"\"\"Calculate DQ metrics and build report.\"\"\"\n",
    "\n",
    "        if self.engine == \"pandas\":\n",
    "            return self._fit_pandas(tables)\n",
    "\n",
    "        if self.engine == \"pyspark\":\n",
    "            return self._fit_pyspark(tables)\n",
    "\n",
    "        raise NotImplementedError(\"Only pandas and pyspark APIs currently supported!\")\n",
    "    \n",
    "    def _fit_pandas(self, tables: Dict[str, pd.DataFrame]) -> Dict:\n",
    "        return self._fit(tables)\n",
    "\n",
    "    def _fit_pyspark(self, tables: Dict[str, ps.DataFrame]) -> Dict:\n",
    "        return self._fit(tables)\n",
    "    \n",
    "    \n",
    "    @memoize\n",
    "    def _fit(self, tables: Dict[str, Union[pd.DataFrame, ps.DataFrame]]) -> Dict:\n",
    "        \"\"\"Initial method fit\"\"\"\n",
    "        self.report_ = {}\n",
    "        report = self.report_\n",
    "\n",
    "        result = []\n",
    "        passed = 0\n",
    "        failed = 0\n",
    "        errors = 0\n",
    "        for table_name, metric, limits in self.checklist:\n",
    "            df = tables[table_name]\n",
    "            error = ''\n",
    "            try:\n",
    "                values = metric(df)\n",
    "                if len(limits) == 0:\n",
    "                    status = '.'\n",
    "                    passed+=1\n",
    "                else:\n",
    "                    # limits: {\"total\": (1, 1e6), \"\": () ... }\n",
    "                    for column_limit, limit in limits.items():\n",
    "                        value = values[column_limit]\n",
    "                        if value >= limit[0] and value <= limit[1]:\n",
    "                            status = '.'\n",
    "                            passed+=1\n",
    "                        else:\n",
    "                            status = 'F'\n",
    "                            failed+=1\n",
    "                  \n",
    "            except Exception as e:\n",
    "                status = 'E'\n",
    "                error = type(e).__name__ \n",
    "                errors+=1\n",
    "        \n",
    "            result.append({'table_name': table_name,\n",
    "                           'metric': str(metric),\n",
    "                           'limits': str(limits),\n",
    "                           'values': values,\n",
    "                           'status': status,\n",
    "                           'error': error})\n",
    "        \n",
    "        \n",
    "        report['title'] = 'DQ Report for tables ' + str(sorted(list(tables.keys())))\n",
    "        report['result'] = pd.DataFrame(result)\n",
    "        report['total'] = len(result)\n",
    "        report['passed'] = passed\n",
    "        report['passed_pct'] = round(passed / report['total'] * 100, 2)\n",
    "        report['failed'] = failed\n",
    "        report['failed_pct'] = round(failed / report['total'] * 100, 2)\n",
    "        report['errors'] = errors\n",
    "        report['errors_pct'] = round(errors / report['total'] * 100, 2)\n",
    "        \n",
    "\n",
    "        return report\n",
    "    \n",
    "\n",
    "    def to_str(self) -> None:\n",
    "        \"\"\"Convert report to string format.\"\"\"\n",
    "        report = self.report_\n",
    "\n",
    "        msg = (\n",
    "            \"This Report instance is not fitted yet. \"\n",
    "            \"Call 'fit' before using this method.\"\n",
    "        )\n",
    "\n",
    "        assert isinstance(report, dict), msg\n",
    "\n",
    "        pd.set_option(\"display.max_rows\", 500)\n",
    "        pd.set_option(\"display.max_columns\", 500)\n",
    "        pd.set_option(\"display.max_colwidth\", 20)\n",
    "        pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "        return (\n",
    "            f\"{report['title']}\\n\\n\"\n",
    "            f\"{report['result']}\\n\\n\"\n",
    "            f\"Passed: {report['passed']} ({report['passed_pct']}%)\\n\"\n",
    "            f\"Failed: {report['failed']} ({report['failed_pct']}%)\\n\"\n",
    "            f\"Errors: {report['errors']} ({report['errors_pct']}%)\\n\"\n",
    "            \"\\n\"\n",
    "            f\"Total: {report['total']}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "784c7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = Report(CHECKLIST, 'pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0ef3e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DQ Report for tables ['relevance', 'sales']\\n\\n   table_name               metric               limits               values status              error\\n0       sales         CountTotal()  {'total': (1, 10...         {'total': 7}      .                   \\n1       sales  CountLag(column=...      {'lag': (0, 3)}  {'today': '2023-...      F                   \\n2       sales  CountDuplicates(...    {'total': (0, 0)}  {'total': 7, 'co...      F                   \\n3       sales  CountNull(column...    {'total': (0, 0)}  {'total': 7, 'co...      F                   \\n4       sales  CountRatioBelow(...  {'delta': (0, 0....  {'total': 7, 'co...      E          NameError\\n5       sales  CountCB(column='...                   {}  {'lcb': 0.0, 'uc...      .                   \\n6       sales  CountZeros(colum...  {'delta': (0, 0.3)}  {'total': 7, 'co...      .                   \\n7       sales  CountBelowValue(...  {'delta': (0, 0.3)}  {'total': 7, 'co...      .                   \\n8   relevance         CountTotal()  {'total': (1, 10...        {'total': 10}      .                   \\n9   relevance  CountLag(column=...      {'lag': (0, 3)}        {'total': 10}      E  AnalysisException\\n10  relevance  CountZeros(colum...  {'delta': (0, 0.2)}  {'total': 10, 'c...      .                   \\n11  relevance  CountZeros(colum...  {'delta': (0, 0.5)}  {'total': 10, 'c...      .                   \\n12  relevance  CountNull(column...  {'delta': (0, 0.1)}  {'total': 10, 'c...      F                   \\n13  relevance  CountBelowValue(...  {'delta': (0, 0.5)}  {'total': 10, 'c...      .                   \\n14  relevance  CountBelowColumn...    {'total': (0, 0)}  {'total': 10, 'c...      F                   \\n15  relevance  CountBelowColumn...    {'total': (0, 0)}  {'total': 10, 'c...      F                   \\n\\nPassed: 8 (50.0%)\\nFailed: 6 (37.5%)\\nErrors: 2 (12.5%)\\n\\nTotal: 16\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.fit({'sales': df_relevance, 'relevance': df})\n",
    "cl.to_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63834cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+------+--------+\n",
      "|       day|item_id|views|clicks|payments|\n",
      "+----------+-------+-----+------+--------+\n",
      "|2022-09-24|    100| NULL|   219|      56|\n",
      "|2022-09-24|    200| 1248|   343|       1|\n",
      "|2022-09-24|    300|  993|   102|      71|\n",
      "|2022-09-23|    100| 3244|   730|      18|\n",
      "|2022-09-23|    200|  830|   203|       9|\n",
      "|2022-09-23|    300|    0|     0|       2|\n",
      "|2022-09-22|    100| NULL|  NULL|      20|\n",
      "|2022-09-22|    200| 5320|   500|      13|\n",
      "|2022-09-22|    300|  777|    68|       2|\n",
      "|2022-09-24|    200| 1248|   343|       1|\n",
      "+----------+-------+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dec9497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan\n",
    "n = df.count()\n",
    "mask_x = ~isnan(col('clicks')) & col('clicks').isNotNull()\n",
    "mask_y = ~isnan(col('views')) & col('views').isNotNull()\n",
    "k = df.filter(mask_x & mask_y & (col('clicks') < col('views'))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db45b103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbc_dict = {'total': 10, 'count': 1, 'delta': 1/10}\n",
    "cbc = CountBelowColumn('views', 'clicks')\n",
    "assert cbc(df) == cbc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ef1462ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- day: string (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- clicks: integer (nullable = true)\n",
      " |-- payments: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
